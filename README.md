# BioCypher KG

A project for creating [BioCypher-driven](https://github.com/biocypher/biocypher) knowledge graphs with multiple output formats.
## Prerequisites

- Python 3.9+  
- [UV](https://github.com/astral-sh/uv) package manager  

## Quick Start (Option 1)

### 1. Clone and Setup
```bash
git clone https://github.com/rejuve-bio/biocypher-kg.git
cd biocypher-kg
make setup
```

### 2. Run the Application

#### Option 1: Interactive Mode (Recommended for new users)
```bash
make run
```
This will guide you through all parameters step by step with sensible defaults.

#### Option 2: Quick Sample Run
```bash
make run-sample WRITER_TYPE=<metta,neo4j,prolog>
```

#### Option 3: Direct Run with Parameters
```bash
make run-direct OUTPUT_DIR=./output \
               ADAPTERS_CONFIG=./config.yaml \
               DBSNP_RSIDS=./rsids.txt \
               DBSNP_POS=./pos.txt \
               WRITER_TYPE=metta \
               WRITE_PROPERTIES=no \
               ADD_PROVENANCE=no
```
### Interactive Mode Example
When you run `make run`, you'll see:
```
ğŸš€ Starting interactive knowledge graph creation...

ğŸ“ Enter output directory [./output]: 
âš™ï¸  Enter adapters config path [./config/adapters_config_sample.yaml]: 
ğŸ§¬ Enter dbSNP RSIDs path [./aux_files/sample_dbsnp_rsids.pkl]: 
ğŸ“ Enter dbSNP positions path [./aux_files/sample_dbsnp_pos.pkl]: 
ğŸ“ Enter writer type (metta/prolog/neo4j) [metta]: 
ğŸ“‹ Write properties? (yes/no) [no]: 
ğŸ”— Add provenance? (yes/no) [no]: 
```

### Available Make Commands
```bash
make help           # Show all commands
make setup          # Install UV and dependencies
make run            # Interactive mode (recommended)
make run-interactive # Same as make run
make run-direct     # Direct mode with parameters
make run-sample     # Run with sample data
make test           # Run tests
make clean          # Clean temporary files
make distclean      # Full clean
```


## BioCypher Knowledge Graph CLI Tool (Option 2)

A user-friendly command line interface for generating knowledge graphs using BioCypher, with support for both Human and Drosophila melanogaster (Fly) data.

### Features

- ğŸ§¬ Human and ğŸª° Fly organism support  
- âš¡ Default configurations for quick start  
- ğŸ› ï¸ Custom configuration options  
- ğŸ“Š Interactive menu system with rich visual interface  
- ğŸ” Multiple output formats (Neo4j, MeTTa, Prolog)  
- ğŸ“ˆ Progress tracking and logging  


### Setup

```bash
# 1. Clone the repository:
git clone https://github.com/rejuve-bio/biocypher-kg.git
cd biocypher-kg

# 2. Install dependencies using UV
uv sync

# 3. Create required directories and run the CLI
mkdir -p output_human output_fly
uv run python biocypher_cli/cli.py

# ğŸ“‚ Project Structure:
# biocypher-kg/
# â”œâ”€â”€ biocypher_cli/            # CLI source code
# â”‚   â””â”€â”€ cli.py
# â”œâ”€â”€ config/                   # Configuration files or (Custom Config files)
# â”‚   â”œâ”€â”€ adapters_config.yaml/adapters_config_sample.yaml
# â”‚   â”œâ”€â”€ dmel_adapters_config.yaml/dmel_adapters_config_sample.yaml
# â”‚   â””â”€â”€ biocypher_config.yaml
# â”œâ”€â”€ aux_files/                # Auxiliary data files (or Custom config files)
# â”‚   â”œâ”€â”€ gene_mapping.pkl/abc_tissues_to_ontology_map.pkl
# â”‚   â””â”€â”€ sample_dbsnp_rsids.pkl
# â”œâ”€â”€ output_human/             # Default human output
# â”œâ”€â”€ output_fly/               # Default fly output
# â””â”€â”€ pyproject.toml            # Dependencies
```

## ğŸ›  Usage

### Structure
The project template is structured as follows:
```
.
.
â”‚ # Project setup
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”‚
â”‚ # Docker setup
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ biocypher_entrypoint_patch.sh
â”‚   â”œâ”€â”€ create_table.sh
â”‚   â””â”€â”€ import.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-variables.env
â”‚
â”‚ # Project pipeline
|â”€â”€ biocypher_metta
â”‚   â”œâ”€â”€ adapters
â”‚   â”œâ”€â”€ metta_writer.py
â”‚   â”œâ”€â”€ prolog_writer.py
â”‚   â””â”€â”€ neo4j_csv_writer.py
â”‚
â”œâ”€â”€ create_knowledge_graph.py
â”‚ 
â”‚ # Scripts
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ metta_space_import.py
â”‚   â”œâ”€â”€ neo4j_loader.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ adapters_config_sample.yaml
â”‚   â”œâ”€â”€ biocypher_config.yaml
â”‚   â”œâ”€â”€ biocypher_docker_config.yaml
â”‚   â”œâ”€â”€ download.yaml
â”‚   â””â”€â”€ schema_config.yaml
â”‚
â”‚ # Downloading data
â”œâ”€â”€ biocypher_dataset_downloader/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ dmel_download_data.py
    â”œâ”€â”€ hsa_download_data.py
    â”œâ”€â”€ download_manager.py
    â”œâ”€â”€ protocols/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ base.py
        â””â”€â”€ http.py
```

The main components of the BioCypher pipeline are the
`create_knowledge_graph.py`, the configuration in the `config` directory, and
the adapter module in the `biocypher_metta` directory. The input adapters are used for preprocessing biomedical
databases and converting them into BioCypher nodes and edges. 

### Writers
The project supports multiple output formats for the knowledge graph:

1. **MeTTa Writer (`metta_writer.py`)**: Generates knowledge graph data in the MeTTa format.
2. **Prolog Writer (`prolog_writer.py`)**: Generates knowledge graph data in the Prolog format.
3. **Neo4j CSV Writer (`neo4j_csv_writer.py`)**: Generates CSV files containing nodes and edges of the knowledge graph, along with Cypher queries to load the data into a Neo4j database.

### Neo4j Loader
To load the generated knowledge graph into a Neo4j database, use the `neo4j_loader.py` script:

```bash
python scripts/neo4j_loader.py --output-dir <path_to_output_directory>
```

#### Neo4j Loader Options
- `--output-dir`: **Required**. Path to the directory containing the generated Cypher query files.
- `--uri`: Optional. Neo4j database URI (default: `bolt://localhost:7687`)
- `--username`: Optional. Neo4j username (default: `neo4j`)

When you run the script, you'll be prompted to enter your Neo4j database password securely.

**Notes:**
- Ensure your Neo4j database is running before executing the loader.
- The script will automatically find and process all Cypher query files (node and edge files) in the specified output directory.
- It supports processing multiple directories containing Cypher files.
- The loader creates constraints and loads data in a single session.
- Logging is provided to help you track the loading process.

## â¬‡ Downloading data
The `downloader` directory contains code for downloading data from various sources.
The `download.yaml` file contains the configuration for the data sources.

To download the data, run the `download_data.py` script with the following command:
```{bash}
python downloader/download_data.py --output_dir <output_directory>
```

To download data from a specific source, run the script with the following command:
```{bash}
python downloader/download_data.py --output_dir <output_directory> --source <source_name>
```
